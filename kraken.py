# -*- coding: utf-8 -*-
"""Kraken.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POqJXt4Qo1XHexJyiB4UGCpbSqq-v_Im

#Kraken Project
##Erfan Mahmoudinia 97212889

In this project we are going to implement Kraken which is refrenced below . As students suggested in piazza we are using ncbi-genome-download to create index file dynamically. we used ete3 package which have taxonomy tree. which we can call a node or family to get the genomes. this process help us to handle ram and storage we need for this project.



Refrence : 
https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-3-r46
"""

!pip install ncbi-genome-download
!pip install ete3
!wget ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank/bacteria/assembly_summary.txt

"""The assembly_summary.txt contain information about the bacteria existed in ncbi database this file is corresponds to the tree we will use from ete3 package. we need assembly_summary file to get taxid of each bacteria. as you can see we show the head of this file."""

import pandas as pd
summary = pd.read_csv("assembly_summary.txt",sep="\t",skiprows=1)
summary.head()

"""For all of the following processes we need tax id mapped to each file name using summary file"""

def file_to_taxId(filename):
  splitted = filename[0:15]
  return int(summary[summary['gbrs_paired_asm'] == splitted]["taxid"])

file_to_taxId("GCF_000014005.1_ASM1400v1_genomic.fasta.gz")

"""NCBITaxa contains the tree we talked in the previous section. which we load it in ncbi variable to use in the following processes"""

from ete3 import NCBITaxa
ncbi = NCBITaxa()

"""here we used get_name_translator to choose the part of the tree that corresponds to bacteria name we give the function then we write the leaves of the sub tree into my_taids.txt"""

bacteria = "acidobacteria"

subTree= ncbi.get_name_translator([bacteria])[bacteria][0]
z = ncbi.get_descendant_taxa(subTree, return_tree=True)

f = open('my_taxids.txt', 'w+')
for pp in z.get_leaves():
    f.write(pp.name + '\n')
f.close()

"""Now with using my_taxids.txt we can download the complete genome of this bacteria"""

!ncbi-genome-download --format fasta --taxid my_taxids.txt --assembly-level complete bacteria

"""And now we create genomes folder to integrate the data into one folder to run the following processes in a easy way."""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# mkdir genomes
# for f in $(find refseq/ -maxdepth 3 -type f); do
# b=$(basename $f)
# if [ ${b: -7} == ".fna.gz" ]; then  
# mv  "$f" "/content/genomes/${b%.fna.gz}.fasta.gz"
# fi  
# done

"""To read fasta and fastq files we need to use biopython package , which installed with pip install"""

!pip install biopython

"""here we set the parameters such as kmer_len and minimizer_len for the consistancy."""

Kmer_len = 31
Minimizer_len = 13

"""definition Reverse Complement Code Which Reverse the sequence and then complement it. complement means 

A --> T

T --> A

C --> G

G --> C
"""

def Reverse_Complement(seq):
  result = ''
  for ch in seq[len(seq)::-1]:
    if ch == 'A':
      result = result + 'T'
    elif ch == 'T':
      result = result + 'A'
    elif ch == 'C':
      result = result + 'G'
    elif ch == 'G':
      result = result + 'C'
  return result

"""we use defaultdict in order to create a dictionary of dictionaries."""

from collections import defaultdict

"""Minimizer Finder is a function which consider Z as an initial minimizer which will be replaced by the first minimizer. 

in this function we get a sequnce(kmer) as an input then run a window in this sequence with length of minimizer length that user defined.

and finally do the same for the reverse complement.
"""

def Minimizer_finder(seq):
  minimizer_optimal = 100000000
  position = -1
  seq_rev = Reverse_Complement(seq)
  for j in range(len(seq)-Minimizer_len):
    minimizer = hash(seq[j:j+Minimizer_len])
    minimizer_rev = hash(seq_rev[j:j+Minimizer_len])
    if minimizer < minimizer_optimal:
      minimizer_optimal = minimizer
      position = j
    if minimizer_rev < minimizer_optimal:
      minimizer_optimal = minimizer_rev
      position = len(seq) - j - Minimizer_len  
  return minimizer_optimal , position

"""The Core of the project is the next section which I named it sequence analyzer. this functin tend to find kmer and correspond minimizer using previous function. 

I designed this function to run optimizely as it run when the last minimizer is done. it means that we just check the last charecters to check if any new minimizer created or not! and check if current minimizer goes out from the window. 


hash function : I used python hash function which is consistant between systems and this hash function cause the uniform distribution (approximatly) and ease our work when we want to find the indeces.
"""

Minimizer_Kmer = defaultdict(dict)

def seq_anlyzer(seq , taxId):
  previous_minimizer = 1000000000
  minimizer_position = -1
  for i in range(len(seq)-Kmer_len):
    kmer = seq[i:i+Kmer_len]
    if minimizer_position < 0:
      previous_minimizer , minimizer_position = Minimizer_finder(kmer)
    else:
      if hash(kmer[Kmer_len-Minimizer_len:Kmer_len]) < previous_minimizer: 
        previous_minimizer , minimizer_position = hash(kmer[Kmer_len-Minimizer_len:Kmer_len]) , Kmer_len-Minimizer_len
      if  hash(Reverse_Complement(kmer[Kmer_len-Minimizer_len:Kmer_len])) < previous_minimizer:
        previous_minimizer , minimizer_position = hash(Reverse_Complement(kmer[Kmer_len-Minimizer_len:Kmer_len])) , Kmer_len-Minimizer_len
    if kmer in Minimizer_Kmer[previous_minimizer]:
      if taxId != Minimizer_Kmer[previous_minimizer][kmer]:
        Minimizer_Kmer[previous_minimizer][kmer] = z.get_common_ancestor([Minimizer_Kmer[previous_minimizer][kmer] , taxId]).name
    else: 
      Minimizer_Kmer[previous_minimizer][kmer] = taxId
    minimizer_position -= 1

"""This part is just for checking the functions"""

seq = "ACCGATGCATCGATCGATCGACTGACTGACTAGCTTCGATCGATCGTACGTACGTGTCAGTCA"
seq_anlyzer(seq , "240015")
print(seq)
print(seq[len(seq)::-1])
print(Reverse_Complement(seq))
# Minimizer_finder(seq)

"""You can see Minimizer_Kmer Dictionary this structure is like below"""

Minimizer_Kmer

"""Now it's time to read the files which we save them in genomes folder and push it into seq_analyzer function in order to create index file."""

from Bio import SeqIO
import os
import gzip
genomes_files = os.listdir("genomes/")


Minimizer_Kmer = defaultdict(dict)

index = 1

for genome_file in genomes_files:
  with gzip.open("genomes/" + genome_file, "rt") as handle:
    fasta_sequences = SeqIO.parse(handle,'fasta')
    taxid = file_to_taxId(genome_file)
    print("======================================")
    print("filename : " , genome_file ," Genome Number : " , index  , " taxid : " , taxid)
    for fasta in fasta_sequences:
        name, sequence = fasta.id, str(fasta.seq)
        seq_anlyzer(sequence , str(taxid))
    index += 1
    print("======================================")

"""find_score is the function which we can calculate the score of each kmer. actullay associate each kmer by using index file. accosiation between kmer and taxids."""

def find_score(seq):
  previous_minimizer = 100000000
  minimizer_position = -1
  for i in range(len(seq)-Kmer_len):
    kmer = seq[i:i+Kmer_len]
    if minimizer_position < 0:
      previous_minimizer , minimizer_position = Minimizer_finder(kmer)
    else:
      if hash(kmer[Kmer_len-Minimizer_len:Kmer_len]) < previous_minimizer: 
        previous_minimizer , minimizer_position = hash(kmer[Kmer_len-Minimizer_len:Kmer_len]) , Kmer_len-Minimizer_len
      if hash(Reverse_Complement(kmer[Kmer_len-Minimizer_len:Kmer_len])) < previous_minimizer:
        previous_minimizer , minimizer_position = hash(Reverse_Complement(kmer[Kmer_len-Minimizer_len:Kmer_len])) , Kmer_len-Minimizer_len

    if kmer in Minimizer_Kmer[previous_minimizer]:
      if Minimizer_Kmer[previous_minimizer][kmer] not in scores:
        scores[Minimizer_Kmer[previous_minimizer][kmer]] = 0
      scores[Minimizer_Kmer[previous_minimizer][kmer]] += 1
    minimizer_position -= 1

"""Then we create a sub topolpogy using ete3 package from the nodes we used. and calculate Root to leaves (RTL) for each node to find the best match."""

import operator

association = dict()

index = 0 
with open("GCA_000014005.1_ASM1400v1_genomic.fna_R1.fastq", "rt") as handle:
    for record in SeqIO.parse(handle, "fastq"):
      print("=============================================")
      print("number : " , index)
      scores = dict()
      find_score(record.seq)
      if len(scores) > 0 :
        tree = ncbi.get_topology(list(scores.keys()))
        main_scores = dict()
        for taxid in list(scores.keys()):
          score = 0 
          node = tree.search_nodes(name=str(taxid))[0]
          ancestors = node.get_ancestors()
          for ancestor in ancestors:
            score += scores[ancestor.name]
          main_scores[str(taxid)] = score
        maxAssociated = max(main_scores.items(), key=operator.itemgetter(1))[0]
        if maxAssociated not in association:
          association[maxAssociated] = 0
        association[maxAssociated] += 1
      index += 1

"""Here is the number of association to each taxid"""

association

"""and the contribution in each taxid calculated as below"""

Contribution = {k: v / total for total in (sum(association.values()),) for k, v in association.items()}

Contribution